{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ReconNER, Debug annotated Named Entity Recognition (NER) data for inconsitencies and get insights on improving the quality of your data. Documentation : https://microsoft.github.io/reconner Source Code : https://github.com/microsoft/reconner ReconNER is a library to help you fix your annotated NER data and identify examples that are hardest for your model to predict so you can strategically prioritize the examples you annotate. The key features are: Data Validation and Cleanup : Easily Validate the format of your NER data. Filter overlapping Entity Annotations, fix missing properties. Model Insights : Analyze how well your model does on your Dataset. Identify the top errors your model is making so you can prioritize data collection and correction strategically. Model Insights : Analyze how well your model does on your Dataset. Identify the top errors your model is making so you can prioritize data collection and correction strategically. Dataset Management : ReconNER provides a Dataset class to manage the train/dev/test split of your data and apply the same functions across all splits in your data + a concatenation of all examples. Operate inplace to consistently transform your data. Serializable Dataset : Serialize and Deserialize your data to and from JSON to the ReconNER type system. Type Hints : Comprehensive Typing system based on Python 3.6+ Type Hints Requirements \u00b6 Python 3.6+ ReconNER is built on a few comprehensive, high-performing packages. spaCy Pydantic (Type system and JSON Serialization) Typer (CLI) . Installation \u00b6 $ pip install reconner ---> 100% Successfully installed reconner License \u00b6 This project is licensed under the terms of the MIT license.","title":"Introduction"},{"location":"#requirements","text":"Python 3.6+ ReconNER is built on a few comprehensive, high-performing packages. spaCy Pydantic (Type system and JSON Serialization) Typer (CLI) .","title":"Requirements"},{"location":"#installation","text":"$ pip install reconner ---> 100% Successfully installed reconner","title":"Installation"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"contributing/","text":"Contributing \u00b6 This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Development - Contributing"},{"location":"contributing/#contributing","text":"This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contributing"},{"location":"features/","text":"Design based on FastAPI \u00b6 Typer is FastAPI 's little sibling. It follows the same design and ideas. If you know FastAPI, you already know Typer ... more or less. Just Modern Python \u00b6 It's all based on standard Python 3.6 type declarations. No new syntax to learn. Just standard modern Python. If you need a 2 minute refresher of how to use Python types (even if you don't use FastAPI or Typer), check the FastAPI tutorial section: Python types intro . Editor support \u00b6 Typer was designed to be easy and intuitive to use, to ensure the best development experience. With autocompletion everywhere. You will rarely need to come back to the docs. Here's how your editor might help you: in Visual Studio Code : in PyCharm : You will get completion for everything. That's something no other CLI library provides right now. No more guessing what type was that variable, if it could be None , etc. Short \u00b6 It has sensible defaults for everything, with optional configurations everywhere. All the parameters can be fine-tuned to do what you need, customize the help, callbacks per parameter, make them required or not, etc. But by default, it all \"just works\" . User friendly CLI apps \u00b6 The resulting CLI apps created with Typer have the nice features of many \"pro\" command line programs you probably already love. Automatic help options for the main CLI program and all the its subcommands. Automatic command and subcommand structure handling (you will see more about subcommands in the Tutorial - User Guide). Automatic autocompletion for the CLI app in all operating systems, in all the shells (Bash, Zsh, Fish, PowerShell), so that the final user of your app can just hit TAB and get the available options or subcommands. * * Autocompletion For the autocompletion to work on all shells you also need to add the dependency click-completion . Just that. And Typer does the rest. If Typer detects click-completion installed, it will automatically create 2 CLI options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. Then you can tell the user to run that command and the rest will just work. The power of Click \u00b6 Click is one of the most popular tools for building CLIs in Python. Typer is based on it, so you get all its benefits, plug-ins, robustness, etc. But you can write simpler code with the benefits of modern Python.","title":"Features"},{"location":"features/#design-based-on-fastapi","text":"Typer is FastAPI 's little sibling. It follows the same design and ideas. If you know FastAPI, you already know Typer ... more or less.","title":"Design based on FastAPI"},{"location":"features/#just-modern-python","text":"It's all based on standard Python 3.6 type declarations. No new syntax to learn. Just standard modern Python. If you need a 2 minute refresher of how to use Python types (even if you don't use FastAPI or Typer), check the FastAPI tutorial section: Python types intro .","title":"Just Modern Python"},{"location":"features/#editor-support","text":"Typer was designed to be easy and intuitive to use, to ensure the best development experience. With autocompletion everywhere. You will rarely need to come back to the docs. Here's how your editor might help you: in Visual Studio Code : in PyCharm : You will get completion for everything. That's something no other CLI library provides right now. No more guessing what type was that variable, if it could be None , etc.","title":"Editor support"},{"location":"features/#short","text":"It has sensible defaults for everything, with optional configurations everywhere. All the parameters can be fine-tuned to do what you need, customize the help, callbacks per parameter, make them required or not, etc. But by default, it all \"just works\" .","title":"Short"},{"location":"features/#user-friendly-cli-apps","text":"The resulting CLI apps created with Typer have the nice features of many \"pro\" command line programs you probably already love. Automatic help options for the main CLI program and all the its subcommands. Automatic command and subcommand structure handling (you will see more about subcommands in the Tutorial - User Guide). Automatic autocompletion for the CLI app in all operating systems, in all the shells (Bash, Zsh, Fish, PowerShell), so that the final user of your app can just hit TAB and get the available options or subcommands. * * Autocompletion For the autocompletion to work on all shells you also need to add the dependency click-completion . Just that. And Typer does the rest. If Typer detects click-completion installed, it will automatically create 2 CLI options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. Then you can tell the user to run that command and the rest will just work.","title":"User friendly CLI apps"},{"location":"features/#the-power-of-click","text":"Click is one of the most popular tools for building CLIs in Python. Typer is based on it, so you get all its benefits, plug-ins, robustness, etc. But you can write simpler code with the benefits of modern Python.","title":"The power of Click"},{"location":"release-notes/","text":"0.0.1 \u00b6 First commit. Publish to PyPI to reserve package name. Initial Functionality for Validation, Insights, Dataset Managment, Serialization and Deserialization from JSON files, Type System","title":"Release Notes"},{"location":"release-notes/#001","text":"First commit. Publish to PyPI to reserve package name. Initial Functionality for Validation, Insights, Dataset Managment, Serialization and Deserialization from JSON files, Type System","title":"0.0.1"},{"location":"api/dataset/","text":"recon.dataset.Dataset \u00b6 recon.dataset.Dataset is a container to run other recon operations across your train/dev/test split The recon.dataset.Dataset.apply function takes any of the other recon functions and runs them on all the datasets in sequence. API \u00b6 class recon. Dataset ( train , dev , test=None ) Container for a full dataset with train/dev/test splits. Used to apply core functions to all datasets at once. Parameters \u00b6 train : (List[Example]), required. List of Examples for train set dev : (List[Example]), required. List of Examples for dev set test : (List[Example], optional), Defaults to None. List of Examples for test set all apply ( self , func , *args , **kwargs ) Apply an existing function to all datasets Parameters \u00b6 func : (Callable[[List[Example]], Any]), required. Function from an existing recon module that can operate on a List of Examples Returns \u00b6 (Dict[str, Any): Dictionary mapping dataset names to return type of func Callable apply_ ( self , func , *args , **kwargs ) Apply an existing function to all data inplace Parameters \u00b6 func : (Callable[[List[Example]], List[Example]]), required. Function from an existing recon module that can operate on a List of Examples dev from_disk ( data_dir , train_file='train.jsonl' , dev_file='dev.jsonl' , test_file='test.jsonl' , loader_func= ) Load Dataset from disk given a directory with files named explicitly train.jsonl, dev.jsonl, and test.jsonl Parameters \u00b6 data_dir : (Path), required. directory to load from train_file : (str, optional), Defaults to train.jsonl. Filename of train data under path dev_file : (str, optional), Defaults to dev.jsonl. Filename of dev data under path test_file : (str, optional), Defaults to test.jsonl. Filename of test data under path loader_func : (Callable, optional), Defaults to read_jsonl. Loader function (TODO: Make this a bit more generic) test train","title":"Dataset"},{"location":"api/dataset/#recondatasetdataset","text":"recon.dataset.Dataset is a container to run other recon operations across your train/dev/test split The recon.dataset.Dataset.apply function takes any of the other recon functions and runs them on all the datasets in sequence.","title":"recon.dataset.Dataset"},{"location":"api/dataset/#api","text":"class recon. Dataset ( train , dev , test=None ) Container for a full dataset with train/dev/test splits. Used to apply core functions to all datasets at once.","title":"API"},{"location":"api/insights/","text":"recon.insights \u00b6 The recon.insights module provides more complex functionality for understanding your dataset. It provides functions for identifying disparities in your annotations and identifying the kinds of examples and labels that are hardest for your model to identify. Some of the functionality in recon.insights require a recon.recognizer.EntityRecognizer object. You can read more about the EntityRecognizer class here: Tutorial - Custom EntityRecognizer API \u00b6 recon.insights. ents_by_label ( data , use_lower=True ) Get a dictionary of unique text spans by label for your data Parameters \u00b6 data : (List[Example]), required. List of Examples use_lower : (bool, optional), Defaults to True. Use the lowercase form of the span text Returns \u00b6 (DefaultDict[str, List[str]]): DefaultDict mapping label to sorted list of the unique spans annotated for that label . recon.insights. get_label_disparities ( data , label1 , label2 , use_lower=True ) Identify annotated spans that have different labels in different examples Parameters \u00b6 data : (List[Example]), required. Input List of Examples label1 : (str), required. First label to compare label2 : (str), required. Second label to compare use_lower : (bool, optional), Defaults to True. Use the lowercase form of the span text in ents_to_label Returns \u00b6 (Set[str]): Set of all unique text spans that overlap between label1 and label2 recon.insights. top_prediction_errors ( ner , data , labels=None , n=None , k=None , exclude_fp=False , exclude_fn=False , verbose=False ) Get a sorted list of examples your model is worst at predicting. Parameters \u00b6 ner : (EntityRecognizer), required. An instance of EntityRecognizer data : (List[Example]), required. List of annotated Examples labels : (List[str], optional), Defaults to None. List of labels to get errors for . Defaults to the labels property of ner . n : (int, optional), Defaults to None. If set, only use the top n examples from data k : (int, optional), Defaults to None. If set, return the top k prediction errors, otherwise the whole list. exclude_fp : (bool, optional), Defaults to False. Flag to exclude False Positive errors. exclude_fn : (bool, optional), Defaults to False. Flag to exclude False Negative errors. verbose : (bool, optional), Defaults to False. Show progress_bar or not Returns \u00b6 (List[PredictionError]): List of Prediction Errors your model is making , sorted by the spans your model has the most trouble with .","title":"Insights"},{"location":"api/insights/#reconinsights","text":"The recon.insights module provides more complex functionality for understanding your dataset. It provides functions for identifying disparities in your annotations and identifying the kinds of examples and labels that are hardest for your model to identify. Some of the functionality in recon.insights require a recon.recognizer.EntityRecognizer object. You can read more about the EntityRecognizer class here: Tutorial - Custom EntityRecognizer","title":"recon.insights"},{"location":"api/insights/#api","text":"recon.insights. ents_by_label ( data , use_lower=True ) Get a dictionary of unique text spans by label for your data","title":"API"},{"location":"api/stats/","text":"recon.stats \u00b6 The stats module implements utility functions for getting statistics for an NER dataset. It's useful for getting a quick overview of your data and checking that you have enough examples for each label (including examples with NO ENTITIES ) API \u00b6 ner_stats \u00b6 recon.stats. ner_stats ( data , serialize=False , no_print=False ) Compute statistics for NER data Parameters \u00b6 data : (List[Example]), required. Data as a List of Examples serialize : (bool, optional), Defaults to False. Serialize to a JSON string for printing no_print : (bool, optional), Defaults to False. Don't print, return serialized string. Requires serialize to be True Returns \u00b6 (List[Example]): List of examples or string if serialize and no_print are both True","title":"Stats"},{"location":"api/stats/#reconstats","text":"The stats module implements utility functions for getting statistics for an NER dataset. It's useful for getting a quick overview of your data and checking that you have enough examples for each label (including examples with NO ENTITIES )","title":"recon.stats"},{"location":"api/stats/#api","text":"","title":"API"},{"location":"api/stats/#ner_stats","text":"recon.stats. ner_stats ( data , serialize=False , no_print=False ) Compute statistics for NER data","title":"ner_stats"},{"location":"api/validation/","text":"recon.validation \u00b6 recon.validation provides a set of utility functions to fix and validate annotations to ensure consistency and no overlapping entities are set. These functions are useful to run before any of the functionality in recon.insights or recon.stats . API \u00b6 json_to_examples \u00b6 recon.validation. json_to_examples ( data ) Convert List of Dicts to List of typed Examples Parameters \u00b6 data : (List[Dict[str, Any]]), required. Input List of Dicts to convert Returns \u00b6 (List[Example]): List of typed Examples fix_annotations_format \u00b6 recon.validation. fix_annotations_format ( data ) Fix annotations format for a consistent dataset Parameters \u00b6 data : (List[Dict[str, Any]]), required. List of Examples Returns \u00b6 (List[Dict[str, Any]]): List of Examples with corrected formatting filter_overlaps \u00b6 recon.validation. filter_overlaps ( data ) Filter overlapping entity spans by picking the longest one. Parameters \u00b6 data : (List[Dict[str, Any]]), required. List of Examples Returns \u00b6 (List[Dict[str, Any]]): List of Examples with fixed overlaps","title":"Validation"},{"location":"api/validation/#reconvalidation","text":"recon.validation provides a set of utility functions to fix and validate annotations to ensure consistency and no overlapping entities are set. These functions are useful to run before any of the functionality in recon.insights or recon.stats .","title":"recon.validation"},{"location":"api/validation/#api","text":"","title":"API"},{"location":"api/validation/#json_to_examples","text":"recon.validation. json_to_examples ( data ) Convert List of Dicts to List of typed Examples","title":"json_to_examples"},{"location":"api/validation/#fix_annotations_format","text":"recon.validation. fix_annotations_format ( data ) Fix annotations format for a consistent dataset","title":"fix_annotations_format"},{"location":"api/validation/#filter_overlaps","text":"recon.validation. filter_overlaps ( data ) Filter overlapping entity spans by picking the longest one.","title":"filter_overlaps"},{"location":"tutorial/custom_entity_recognizer/","text":"Custom EntityRecognizer \u00b6","title":"Custom EntityRecognizer"},{"location":"tutorial/custom_entity_recognizer/#custom-entityrecognizer","text":"","title":"Custom EntityRecognizer"},{"location":"tutorial/dataset_apply/","text":"Tutorial - Using Dataset.apply \u00b6 In the previous step, we used the stats.ner_stats function to some stats on our train_data. Now, we want to be able to get these same stats across our train/dev/test split. SO, ReconNER's Dataset class provides a useful method called apply that takes a Callable as a parameter that can run on a list of Example s (e.g. stats.ner_stats ) and run that Callable over all the datasets as well as a concatenation of all the datasets so you get the full picture. Tip You can pass arbitary *args and **kwargs to Dataset.apply and they will be passed along to the callable you provide as the required argument. Update script to use Dataset.apply \u00b6 Let's edit that main.py file you created in the previous step to utilize the Dataset.apply method. from pathlib import Path import srsly import typer from recon.dataset import Dataset from recon.stats import ner_stats def main ( data_dir : Path ): ds = Dataset . from_disk ( data_dir ) ds_stats = ds . apply ( ner_stats , serialize = True , no_print = True ) for name , stats in ds_stats . items (): print ( f \" {name} \" ) print ( '=' * 50 ) print ( stats ) if __name__ == \"__main__\" : typer . run ( main ) Run the application \u00b6 Now, if you run your script you should get the following output: $ python main.py ./examples/data train ================================================== { \"n_examples\":102, \"n_examples_no_entities\":29, \"ents_per_type\":{ \"SKILL\":191, \"PRODUCT\":34, \"JOB_ROLE\":5 } } dev ================================================== { \"n_examples\":110, \"n_examples_no_entities\":49, \"ents_per_type\":{ \"SKILL\":159, \"PRODUCT\":20, \"JOB_ROLE\":1 } } test ================================================== { \"n_examples\":96, \"n_examples_no_entities\":38, \"ents_per_type\":{ \"PRODUCT\":35, \"SKILL\":107, \"JOB_ROLE\":2 } } all ================================================== { \"n_examples\":308, \"n_examples_no_entities\":116, \"ents_per_type\":{ \"SKILL\":457, \"PRODUCT\":89, \"JOB_ROLE\":8 } } Analyzing the results \u00b6 Now that we have a good understanding of the distribution of labels in across our train/dev/test split as well as the summation of all those numbers to the \"all\" data, we can start to see some issues. 1. Not enough JOB_ROLE annotations \u00b6 We clearly don't have enough annotations of the JOB_ROLE in our data. There's no way an NER model could learn to capture JOB_ROLE in a generic way with only 8 total annotations. 2. Barely enough PRODUCT annotations \u00b6 We're also a little low (though not nearly as much) on our PRODUCT label. What to do from here \u00b6 We want our final model to be equally good at extracting these 3 labels of SKILL , PRODUCT and JOB_ROLE so we now know exactly where to invest more time in our annotations effort: getting more examples of JOB_ROLE . Note This is a VERY small dataset sampled from a much larger NER dataset that's powering part of our work on the new v3 Text Analytics Cognitive Service . So here's your glimpse into how we work with data at Microsoft. Until we fix the lack of annotations for the JOB_ROLE label we won't be launching it in production. Next Steps \u00b6 We've only scratched the surface of ReconNER. It's great to have these global stats about our dataset so we can track trends and make sure we're trending in the right direction as we annotate more data. But this data doesn't debug the data we already have. 34 of our 191 SKILL annotations in our train set might actually be instances where JOB_ROLE or PRODUCT is more appropriate. We might have subsets of our data annotated by different people that had a slightly different understanding of the annotation requirements. In the next step of this tutorial we'll dive into the insights module of ReconNER to examine the quality of our existing annotations.","title":"Dataset Apply"},{"location":"tutorial/dataset_apply/#tutorial-using-datasetapply","text":"In the previous step, we used the stats.ner_stats function to some stats on our train_data. Now, we want to be able to get these same stats across our train/dev/test split. SO, ReconNER's Dataset class provides a useful method called apply that takes a Callable as a parameter that can run on a list of Example s (e.g. stats.ner_stats ) and run that Callable over all the datasets as well as a concatenation of all the datasets so you get the full picture. Tip You can pass arbitary *args and **kwargs to Dataset.apply and they will be passed along to the callable you provide as the required argument.","title":"Tutorial - Using Dataset.apply"},{"location":"tutorial/dataset_apply/#update-script-to-use-datasetapply","text":"Let's edit that main.py file you created in the previous step to utilize the Dataset.apply method. from pathlib import Path import srsly import typer from recon.dataset import Dataset from recon.stats import ner_stats def main ( data_dir : Path ): ds = Dataset . from_disk ( data_dir ) ds_stats = ds . apply ( ner_stats , serialize = True , no_print = True ) for name , stats in ds_stats . items (): print ( f \" {name} \" ) print ( '=' * 50 ) print ( stats ) if __name__ == \"__main__\" : typer . run ( main )","title":"Update script to use Dataset.apply"},{"location":"tutorial/dataset_apply/#run-the-application","text":"Now, if you run your script you should get the following output: $ python main.py ./examples/data train ================================================== { \"n_examples\":102, \"n_examples_no_entities\":29, \"ents_per_type\":{ \"SKILL\":191, \"PRODUCT\":34, \"JOB_ROLE\":5 } } dev ================================================== { \"n_examples\":110, \"n_examples_no_entities\":49, \"ents_per_type\":{ \"SKILL\":159, \"PRODUCT\":20, \"JOB_ROLE\":1 } } test ================================================== { \"n_examples\":96, \"n_examples_no_entities\":38, \"ents_per_type\":{ \"PRODUCT\":35, \"SKILL\":107, \"JOB_ROLE\":2 } } all ================================================== { \"n_examples\":308, \"n_examples_no_entities\":116, \"ents_per_type\":{ \"SKILL\":457, \"PRODUCT\":89, \"JOB_ROLE\":8 } }","title":"Run the application"},{"location":"tutorial/dataset_apply/#analyzing-the-results","text":"Now that we have a good understanding of the distribution of labels in across our train/dev/test split as well as the summation of all those numbers to the \"all\" data, we can start to see some issues.","title":"Analyzing the results"},{"location":"tutorial/dataset_apply/#1-not-enough-job_role-annotations","text":"We clearly don't have enough annotations of the JOB_ROLE in our data. There's no way an NER model could learn to capture JOB_ROLE in a generic way with only 8 total annotations.","title":"1. Not enough JOB_ROLE annotations"},{"location":"tutorial/dataset_apply/#2-barely-enough-product-annotations","text":"We're also a little low (though not nearly as much) on our PRODUCT label.","title":"2. Barely enough PRODUCT annotations"},{"location":"tutorial/dataset_apply/#what-to-do-from-here","text":"We want our final model to be equally good at extracting these 3 labels of SKILL , PRODUCT and JOB_ROLE so we now know exactly where to invest more time in our annotations effort: getting more examples of JOB_ROLE . Note This is a VERY small dataset sampled from a much larger NER dataset that's powering part of our work on the new v3 Text Analytics Cognitive Service . So here's your glimpse into how we work with data at Microsoft. Until we fix the lack of annotations for the JOB_ROLE label we won't be launching it in production.","title":"What to do from here"},{"location":"tutorial/dataset_apply/#next-steps","text":"We've only scratched the surface of ReconNER. It's great to have these global stats about our dataset so we can track trends and make sure we're trending in the right direction as we annotate more data. But this data doesn't debug the data we already have. 34 of our 191 SKILL annotations in our train set might actually be instances where JOB_ROLE or PRODUCT is more appropriate. We might have subsets of our data annotated by different people that had a slightly different understanding of the annotation requirements. In the next step of this tutorial we'll dive into the insights module of ReconNER to examine the quality of our existing annotations.","title":"Next Steps"},{"location":"tutorial/getting_insights/","text":"Tutorial - Getting Insights \u00b6 Now that we know how to read","title":"Getting Insights"},{"location":"tutorial/getting_insights/#tutorial-getting-insights","text":"Now that we know how to read","title":"Tutorial - Getting Insights"},{"location":"tutorial/loading_data/","text":"Loading your data \u00b6 ReconNER expects your data to be in the Prodigy Annotation Format . A single example in this format looks like: { \"text\" : \"Apple updates its analytics service with new metrics\" , \"spans\" : [{ \"start\" : 0 , \"end\" : 5 , \"label\" : \"ORG\" }] } ReconNER expects your data to be in a collection in the .jsonl File Format. Load Dataset from_disk \u00b6 There are several utilities available for loading your data. The easiest way to load your data is to initialize a Dataset from disk. If you have a train/dev/test split or just train/dev files in the same directory, it's as easy as calling the from_disk classmethod for the Dataset object. ds = Dataset . from_disk ( 'path/to/data_dir' ) Dataset.from_disk will look in the data_dir you provide for a file structure that looks like: data_dir \u2502 train.jsonl \u2502 dev.jsonl \u2502 test.jsonl Note The test.jsonl file is optional but generally you should split your annotated data into train/dev/test files. The Process of Loading Data \u00b6 While it's recommended to load data using the Dataset.from_disk method, you can also load data directly from disk using the loaders.read_jsonl and loaders.read_json functions. These functions expect the same example format (in fact, the Dataset.from_disk runs loaders.read_jsonl function) and run a few steps. 1. Read data from disk \u00b6 Loads your data with srsly using srsly.read_jsonl or srsly.read_json 2. Fix Annotation Format \u00b6 Fixes some common issues in Annotation formatting that can arise using the validation.fix_annotations_format 3. Filter Overlapping Entities \u00b6 Often, you'll find your data has overlapping entities. For instance, imagine you have 2 annotators and one decided \"Tesla\" is a PRODUCT and the other noticed that the sentence is actually about \"Tesla Motors\" which they label as an ORG . This function does it's best to resolve these overlaps and in the case above would select \"Tesla Motors\" ORG as the correct entity, deleting \"Tesla\" PRODUCT from the data validation.filter_overlaps 4. Load into ReconNER type system \u00b6 Finally these loaders will take a list of JSON examples in the Prodigy Annotation Format outlined above and convert it into a list of Example models using Pydantic Next Steps \u00b6 Once you have your data loaded, you can run other ReconNER functions on top of it to gain insights into the quality and completeness of your NER data","title":"Loading your data"},{"location":"tutorial/loading_data/#loading-your-data","text":"ReconNER expects your data to be in the Prodigy Annotation Format . A single example in this format looks like: { \"text\" : \"Apple updates its analytics service with new metrics\" , \"spans\" : [{ \"start\" : 0 , \"end\" : 5 , \"label\" : \"ORG\" }] } ReconNER expects your data to be in a collection in the .jsonl File Format.","title":"Loading your data"},{"location":"tutorial/loading_data/#load-dataset-from_disk","text":"There are several utilities available for loading your data. The easiest way to load your data is to initialize a Dataset from disk. If you have a train/dev/test split or just train/dev files in the same directory, it's as easy as calling the from_disk classmethod for the Dataset object. ds = Dataset . from_disk ( 'path/to/data_dir' ) Dataset.from_disk will look in the data_dir you provide for a file structure that looks like: data_dir \u2502 train.jsonl \u2502 dev.jsonl \u2502 test.jsonl Note The test.jsonl file is optional but generally you should split your annotated data into train/dev/test files.","title":"Load Dataset from_disk"},{"location":"tutorial/loading_data/#the-process-of-loading-data","text":"While it's recommended to load data using the Dataset.from_disk method, you can also load data directly from disk using the loaders.read_jsonl and loaders.read_json functions. These functions expect the same example format (in fact, the Dataset.from_disk runs loaders.read_jsonl function) and run a few steps.","title":"The Process of Loading Data"},{"location":"tutorial/loading_data/#1-read-data-from-disk","text":"Loads your data with srsly using srsly.read_jsonl or srsly.read_json","title":"1. Read data from disk"},{"location":"tutorial/loading_data/#2-fix-annotation-format","text":"Fixes some common issues in Annotation formatting that can arise using the validation.fix_annotations_format","title":"2. Fix Annotation Format"},{"location":"tutorial/loading_data/#3-filter-overlapping-entities","text":"Often, you'll find your data has overlapping entities. For instance, imagine you have 2 annotators and one decided \"Tesla\" is a PRODUCT and the other noticed that the sentence is actually about \"Tesla Motors\" which they label as an ORG . This function does it's best to resolve these overlaps and in the case above would select \"Tesla Motors\" ORG as the correct entity, deleting \"Tesla\" PRODUCT from the data validation.filter_overlaps","title":"3. Filter Overlapping Entities"},{"location":"tutorial/loading_data/#4-load-into-reconner-type-system","text":"Finally these loaders will take a list of JSON examples in the Prodigy Annotation Format outlined above and convert it into a list of Example models using Pydantic","title":"4. Load into ReconNER type system"},{"location":"tutorial/loading_data/#next-steps","text":"Once you have your data loaded, you can run other ReconNER functions on top of it to gain insights into the quality and completeness of your NER data","title":"Next Steps"},{"location":"tutorial/ner_stats/","text":"Tutorial - NER Statistics \u00b6 Getting statistics about your NER data is extremely helpful throughout the annotation process. It helps you ensure that you're spendind time on the right annotations and that you have enough examples for each type as well as enough examples with NO ENTITIES at all (this is often overlooked but VERY important to build a model that generalizes well). Once you have your data loaded either by itself as a list of Example s or as a Dataset you can easily get statistics using the stats.ner_stats function. The stats.ner_stats function expects a List[Example] as it's input parameter and will return a serializable response with info about your data. Let's see how this works on the provided example data. Error Fix this, add CLI for example data download Example \u00b6 Create a file main.py with: from pathlib import Path import typer from recon.dataset import Dataset from recon.stats import ner_stats def main ( data_dir : Path ): ds = Dataset . from_disk ( data_dir ) train_stats = ner_stats ( ds . train ) ner_stats ( ds . train , serialize = True ) if __name__ == \"__main__\" : typer . run ( main ) Run the application with the example data. $ python main.py ./examples/data/skills { \"n_examples\":102, \"n_examples_no_entities\":29, \"ents_per_type\":{ \"SKILL\":191, \"PRODUCT\":34, \"JOB_ROLE\":5 } } But it isn't super helpful to have stats on just your training data. And it'd be really annoying to have to call the same function on each dataset: ner_stats ( ds . train , serialize = True ) ner_stats ( ds . dev , serialize = True ) ner_stats ( ds . test , serialize = True ) Next Steps \u00b6 In the next step step of this tutorial you'll learn about how to remove the above boilerplate and run functions across your train/dev/test Dataset split.","title":"NER Stats"},{"location":"tutorial/ner_stats/#tutorial-ner-statistics","text":"Getting statistics about your NER data is extremely helpful throughout the annotation process. It helps you ensure that you're spendind time on the right annotations and that you have enough examples for each type as well as enough examples with NO ENTITIES at all (this is often overlooked but VERY important to build a model that generalizes well). Once you have your data loaded either by itself as a list of Example s or as a Dataset you can easily get statistics using the stats.ner_stats function. The stats.ner_stats function expects a List[Example] as it's input parameter and will return a serializable response with info about your data. Let's see how this works on the provided example data. Error Fix this, add CLI for example data download","title":"Tutorial - NER Statistics"},{"location":"tutorial/ner_stats/#example","text":"Create a file main.py with: from pathlib import Path import typer from recon.dataset import Dataset from recon.stats import ner_stats def main ( data_dir : Path ): ds = Dataset . from_disk ( data_dir ) train_stats = ner_stats ( ds . train ) ner_stats ( ds . train , serialize = True ) if __name__ == \"__main__\" : typer . run ( main ) Run the application with the example data. $ python main.py ./examples/data/skills { \"n_examples\":102, \"n_examples_no_entities\":29, \"ents_per_type\":{ \"SKILL\":191, \"PRODUCT\":34, \"JOB_ROLE\":5 } } But it isn't super helpful to have stats on just your training data. And it'd be really annoying to have to call the same function on each dataset: ner_stats ( ds . train , serialize = True ) ner_stats ( ds . dev , serialize = True ) ner_stats ( ds . test , serialize = True )","title":"Example"},{"location":"tutorial/ner_stats/#next-steps","text":"In the next step step of this tutorial you'll learn about how to remove the above boilerplate and run functions across your train/dev/test Dataset split.","title":"Next Steps"}]}